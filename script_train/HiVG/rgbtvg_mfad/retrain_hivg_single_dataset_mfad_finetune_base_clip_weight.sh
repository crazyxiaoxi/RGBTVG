# mfad train from clip weight
DATA_SET="rgbtvg_mfad"
IMGSIZE=${IMGSIZE:-224}
BATCHSIZE=${BATCHSIZE:-32}
MODALITY=${MODALITY:-rgbt}
CUDADEVICES=${CUDADEVICES:-0}
RETRAIN=${RETRAIN:-"../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/mixup/fixed_best_checkpoint.pth"}
NPROC_PER_NODE=$(echo "$CUDADEVICES" | tr ',' '\n' | wc -l | awk '{print $1}')
DIST_CMD=(env CUDA_VISIBLE_DEVICES=$CUDADEVICES python -m torch.distributed.launch --nproc_per_node=$NPROC_PER_NODE --use_env)
DATA_ROOT="../dataset_and_pretrain_model/datasets/VG/image_data"  
SPLIT_ROOT="../dataset_and_pretrain_model/datasets/VG/ref_data_shuffled"  
CLIP_MODEL="../dataset_and_pretrain_model/pretrain_model/pretrained_weights/CLIP/clip_b_ml_cascade_maskrcnn_model_224.pth"  
OUTPUT_DIR_WARMUP="./output_retraining/HiVG_${IMGSIZE}_${MODALITY}/$DATA_SET/rgbt_finetuning_base_clip_weight/output_v100"
OUTPUT_DIR_STAGE1="./output_retraining/HiVG_${IMGSIZE}_${MODALITY}/$DATA_SET/rgbt_finetuning_base_clip_weight/output_v101"
OUTPUT_DIR_STAGE2="./output_retraining/HiVG_${IMGSIZE}_${MODALITY}/$DATA_SET/rgbt_finetuning_base_clip_weight/output_v102"
OUTPUT_DIR_STAGE3="./output_retraining/HiVG_${IMGSIZE}_${MODALITY}/$DATA_SET/rgbt_finetuning_base_clip_weight/output_v103"

echo -e "\n\n\n\n\n\n\n==================== mfad warmup ==========================="
"${DIST_CMD[@]}" --master_port 28887 train_val/hivg_train.py --modality $MODALITY --num_workers 4 --epochs 60  --batch_size $BATCHSIZE --lr 0.00025   --lr_scheduler cosine --aug_crop --aug_scale --aug_translate  --vl_hidden_dim 512  --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --dataset $DATA_SET      --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss   --data_root $DATA_ROOT  --split_root $SPLIT_ROOT   --output_dir $OUTPUT_DIR_WARMUP  --retrain $RETRAIN;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss --save_hilora_clip --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_WARMUP/best_checkpoint.pth --eval_set val      --output_dir $OUTPUT_DIR_WARMUP;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss --save_hilora_clip --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_WARMUP/best_checkpoint.pth --eval_set test     --output_dir $OUTPUT_DIR_WARMUP;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss --save_hilora_clip --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_WARMUP/best_checkpoint.pth --eval_set testA    --output_dir $OUTPUT_DIR_WARMUP;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss --save_hilora_clip --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_WARMUP/best_checkpoint.pth --eval_set testB    --output_dir $OUTPUT_DIR_WARMUP;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss --save_hilora_clip --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_WARMUP/best_checkpoint.pth --eval_set testC    --output_dir $OUTPUT_DIR_WARMUP;
--data_root "../dataset_and_pretrain_model/datasets/VG/image_data"   --split_root "../dataset_and_pretrain_model/datasets/VG/ref_data_shuffled"   --retrain ../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/mixup/fixed_best_checkpoint.pth     --output_dir ./debug
##
# stage 1
echo -e "\n\n\n\n\n\n\n==================== mfad stage 1 ==========================="
"${DIST_CMD[@]}" --master_port 28887 train_val/hivg_train.py --modality $MODALITY --num_workers 4 --epochs 20  --batch_size $BATCHSIZE --lr 0.00010   --lr_scheduler cosine --aug_crop --aug_scale --aug_translate   --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --dataset $DATA_SET      --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss  --save_hilora_clip --hi_lora_stage 1 --mixup_pretrain --data_root $DATA_ROOT  --split_root $SPLIT_ROOT  --hi_lora_retrain $OUTPUT_DIR_WARMUP/best_checkpoint.pth      --hi_lora_clip $OUTPUT_DIR_WARMUP/clip_lora_stage_with_bridge.pth       --output_dir $OUTPUT_DIR_STAGE1/     --sup_type full;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 1 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE1/best_checkpoint.pth      --eval_set val    --output_dir $OUTPUT_DIR_STAGE1;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 1 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE1/best_checkpoint.pth      --eval_set test   --output_dir $OUTPUT_DIR_STAGE1;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 1 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE1/best_checkpoint.pth      --eval_set testA  --output_dir $OUTPUT_DIR_STAGE1;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 1 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE1/best_checkpoint.pth      --eval_set testB  --output_dir $OUTPUT_DIR_STAGE1;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 1 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE1/best_checkpoint.pth      --eval_set testC  --output_dir $OUTPUT_DIR_STAGE1;

#
# stage 2
echo -e "\n\n\n\n\n\n\n==================== mfad stage 2 ==========================="
"${DIST_CMD[@]}" --master_port 28887 train_val/hivg_train.py --modality $MODALITY --num_workers 4 --epochs 20  --batch_size $BATCHSIZE --lr 0.00005   --lr_scheduler cosine --aug_crop --aug_scale --aug_translate   --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --dataset $DATA_SET      --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss  --save_hilora_clip --hi_lora_stage 2 --mixup_pretrain --data_root $DATA_ROOT  --split_root $SPLIT_ROOT  --hi_lora_retrain $OUTPUT_DIR_STAGE1/best_checkpoint.pth      --hi_lora_clip $OUTPUT_DIR_STAGE1/clip_lora_stage_with_bridge.pth       --output_dir $OUTPUT_DIR_STAGE2/     --sup_type full;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 2 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE2/best_checkpoint.pth      --eval_set val    --output_dir $OUTPUT_DIR_STAGE2;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 2 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE2/best_checkpoint.pth      --eval_set test   --output_dir $OUTPUT_DIR_STAGE2;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 2 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE2/best_checkpoint.pth      --eval_set testA  --output_dir $OUTPUT_DIR_STAGE2;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 2 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE2/best_checkpoint.pth      --eval_set testB  --output_dir $OUTPUT_DIR_STAGE2;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 2 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE2/best_checkpoint.pth      --eval_set testC  --output_dir $OUTPUT_DIR_STAGE2;

#
# stage 3
echo -e "\n\n\n\n\n\n\n==================== mfad stage 3 ==========================="
"${DIST_CMD[@]}" --master_port 28887 train_val/hivg_train.py --modality $MODALITY --num_workers 4 --epochs 20  --batch_size $BATCHSIZE --lr 0.000025  --lr_scheduler cosine --aug_crop --aug_scale --aug_translate   --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --dataset $DATA_SET      --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss  --save_hilora_clip --hi_lora_stage 3 --mixup_pretrain --data_root $DATA_ROOT  --split_root $SPLIT_ROOT  --hi_lora_retrain $OUTPUT_DIR_STAGE2/best_checkpoint.pth      --hi_lora_clip $OUTPUT_DIR_STAGE2/clip_lora_stage_with_bridge.pth       --output_dir $OUTPUT_DIR_STAGE3/     --sup_type full;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 3 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE3/best_checkpoint.pth      --eval_set val    --output_dir $OUTPUT_DIR_STAGE3;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 3 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE3/best_checkpoint.pth      --eval_set test   --output_dir $OUTPUT_DIR_STAGE3;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 3 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE3/best_checkpoint.pth      --eval_set testA  --output_dir $OUTPUT_DIR_STAGE3;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 3 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE3/best_checkpoint.pth      --eval_set testB  --output_dir $OUTPUT_DIR_STAGE3;
"${DIST_CMD[@]}" --master_port 28888 train_val/hivg_eval.py --modality $MODALITY --num_workers 2 --batch_size $BATCHSIZE  --dataset $DATA_SET            --imsize $IMGSIZE --max_query_len 77 --normalize_before --mixup_pretrain    --use_mask_loss  --save_hilora_clip --hi_lora_stage 3 --data_root $DATA_ROOT --split_root $SPLIT_ROOT --eval_model $OUTPUT_DIR_STAGE3/best_checkpoint.pth      --eval_set testC  --output_dir $OUTPUT_DIR_STAGE3;
