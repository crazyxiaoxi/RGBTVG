/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:19:04,264] torch.distributed.run: [WARNING] 
[2025-11-11 09:19:04,264] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:19:04,264] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:19:04,264] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-L/14
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.

extract vision layer:  [6, 12, 18, 24]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [4, 10, 16, 22]
trainable params: 9,699,328 || all params: 491,078,913 || trainable%: 1.9751
trainable params: 0 || all params: 491,078,913 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  4.cross_norm.weight
param name:  4.cross_norm.bias
param name:  4.cross_attn.k_proj.base_layer.weight
param name:  4.cross_attn.k_proj.base_layer.bias
param name:  4.cross_attn.k_proj.lora_A.default.weight
param name:  4.cross_attn.k_proj.lora_B.default.weight
param name:  4.cross_attn.v_proj.base_layer.weight
param name:  4.cross_attn.v_proj.base_layer.bias
param name:  4.cross_attn.v_proj.lora_A.default.weight
param name:  4.cross_attn.v_proj.lora_B.default.weight
param name:  4.cross_attn.q_proj.base_layer.weight
param name:  4.cross_attn.q_proj.base_layer.bias
param name:  4.cross_attn.q_proj.lora_A.default.weight
param name:  4.cross_attn.q_proj.lora_B.default.weight
param name:  4.cross_attn.out_proj.base_layer.weight
param name:  4.cross_attn.out_proj.base_layer.bias
param name:  4.cross_attn.out_proj.lora_A.default.weight
param name:  4.cross_attn.out_proj.lora_B.default.weight
param name:  4.cross_mlp.fc1.weight
param name:  4.cross_mlp.fc1.bias
param name:  4.cross_mlp.fc2.weight
param name:  4.cross_mlp.fc2.bias
param name:  4.cross_gate.weight
param name:  4.cross_gate.bias
param name:  4.cross_adaptive_weights.0.weight
param name:  10.cross_norm.weight
param name:  10.cross_norm.bias
param name:  10.cross_attn.k_proj.base_layer.weight
param name:  10.cross_attn.k_proj.base_layer.bias
param name:  10.cross_attn.k_proj.lora_A.default.weight
param name:  10.cross_attn.k_proj.lora_B.default.weight
param name:  10.cross_attn.v_proj.base_layer.weight
param name:  10.cross_attn.v_proj.base_layer.bias
param name:  10.cross_attn.v_proj.lora_A.default.weight
param name:  10.cross_attn.v_proj.lora_B.default.weight
param name:  10.cross_attn.q_proj.base_layer.weight
param name:  10.cross_attn.q_proj.base_layer.bias
param name:  10.cross_attn.q_proj.lora_A.default.weight
param name:  10.cross_attn.q_proj.lora_B.default.weight
param name:  10.cross_attn.out_proj.base_layer.weight
param name:  10.cross_attn.out_proj.base_layer.bias
param name:  10.cross_attn.out_proj.lora_A.default.weight
param name:  10.cross_attn.out_proj.lora_B.default.weight
param name:  10.cross_mlp.fc1.weight
param name:  10.cross_mlp.fc1.bias
param name:  10.cross_mlp.fc2.weight
param name:  10.cross_mlp.fc2.bias
param name:  10.cross_gate.weight
param name:  10.cross_gate.bias
param name:  10.cross_adaptive_weights.0.weight
param name:  16.cross_norm.weight
param name:  16.cross_norm.bias
param name:  16.cross_attn.k_proj.base_layer.weight
param name:  16.cross_attn.k_proj.base_layer.bias
param name:  16.cross_attn.k_proj.lora_A.default.weight
param name:  16.cross_attn.k_proj.lora_B.default.weight
param name:  16.cross_attn.v_proj.base_layer.weight
param name:  16.cross_attn.v_proj.base_layer.bias
param name:  16.cross_attn.v_proj.lora_A.default.weight
param name:  16.cross_attn.v_proj.lora_B.default.weight
param name:  16.cross_attn.q_proj.base_layer.weight
param name:  16.cross_attn.q_proj.base_layer.bias
param name:  16.cross_attn.q_proj.lora_A.default.weight
param name:  16.cross_attn.q_proj.lora_B.default.weight
param name:  16.cross_attn.out_proj.base_layer.weight
param name:  16.cross_attn.out_proj.base_layer.bias
param name:  16.cross_attn.out_proj.lora_A.default.weight
param name:  16.cross_attn.out_proj.lora_B.default.weight
param name:  16.cross_mlp.fc1.weight
param name:  16.cross_mlp.fc1.bias
param name:  16.cross_mlp.fc2.weight
param name:  16.cross_mlp.fc2.bias
param name:  16.cross_gate.weight
param name:  16.cross_gate.bias
param name:  16.cross_adaptive_weights.0.weight
param name:  22.cross_norm.weight
param name:  22.cross_norm.bias
param name:  22.cross_attn.k_proj.base_layer.weight
param name:  22.cross_attn.k_proj.base_layer.bias
param name:  22.cross_attn.k_proj.lora_A.default.weight
param name:  22.cross_attn.k_proj.lora_B.default.weight
param name:  22.cross_attn.v_proj.base_layer.weight
param name:  22.cross_attn.v_proj.base_layer.bias
param name:  22.cross_attn.v_proj.lora_A.default.weight
param name:  22.cross_attn.v_proj.lora_B.default.weight
param name:  22.cross_attn.q_proj.base_layer.weight
param name:  22.cross_attn.q_proj.base_layer.bias
param name:  22.cross_attn.q_proj.lora_A.default.weight
param name:  22.cross_attn.q_proj.lora_B.default.weight
param name:  22.cross_attn.out_proj.base_layer.weight
param name:  22.cross_attn.out_proj.base_layer.bias
param name:  22.cross_attn.out_proj.lora_A.default.weight
param name:  22.cross_attn.out_proj.lora_B.default.weight
param name:  22.cross_mlp.fc1.weight
param name:  22.cross_mlp.fc1.bias
param name:  22.cross_mlp.fc2.weight
param name:  22.cross_mlp.fc2.bias
param name:  22.cross_gate.weight
param name:  22.cross_gate.bias
param name:  22.cross_adaptive_weights.0.weight
trainable params: 54,811,648 || all params: 491,078,913 || trainable%: 11.1615

normalize_before:  True
number of all params:  543390725
Missing keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias', 'ml_text_feat_perceiver.weight', 'ml_text_feat_perceiver.bias']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.bias']
Current model training epoch is:  54
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [00:02<00:04,  2.13s/it] 33%|      | 1/3 [00:02<00:04,  2.34s/it] 67%|   | 2/3 [00:02<00:01,  1.10s/it]100%|| 3/3 [00:02<00:00,  1.51it/s] 67%|   | 2/3 [00:02<00:01,  1.20s/it]100%|| 3/3 [00:02<00:00,  1.07it/s]
100%|| 3/3 [00:02<00:00,  1.38it/s]100%|| 3/3 [00:03<00:00,  1.01s/it]
accuracy2:  0.24404761904761904
Testing time 0:00:03
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_large/best_checkpoint.pth', 'val_set_accuracy': 0.24404761904761904}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:19:26,352] torch.distributed.run: [WARNING] 
[2025-11-11 09:19:26,352] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:19:26,352] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:19:26,352] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 1): env://
| distributed init (rank 0): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-L/14
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.

extract vision layer:  [6, 12, 18, 24]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [4, 10, 16, 22]
trainable params: 9,699,328 || all params: 491,078,913 || trainable%: 1.9751
trainable params: 0 || all params: 491,078,913 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  4.cross_norm.weight
param name:  4.cross_norm.bias
param name:  4.cross_attn.k_proj.base_layer.weight
param name:  4.cross_attn.k_proj.base_layer.bias
param name:  4.cross_attn.k_proj.lora_A.default.weight
param name:  4.cross_attn.k_proj.lora_B.default.weight
param name:  4.cross_attn.v_proj.base_layer.weight
param name:  4.cross_attn.v_proj.base_layer.bias
param name:  4.cross_attn.v_proj.lora_A.default.weight
param name:  4.cross_attn.v_proj.lora_B.default.weight
param name:  4.cross_attn.q_proj.base_layer.weight
param name:  4.cross_attn.q_proj.base_layer.bias
param name:  4.cross_attn.q_proj.lora_A.default.weight
param name:  4.cross_attn.q_proj.lora_B.default.weight
param name:  4.cross_attn.out_proj.base_layer.weight
param name:  4.cross_attn.out_proj.base_layer.bias
param name:  4.cross_attn.out_proj.lora_A.default.weight
param name:  4.cross_attn.out_proj.lora_B.default.weight
param name:  4.cross_mlp.fc1.weight
param name:  4.cross_mlp.fc1.bias
param name:  4.cross_mlp.fc2.weight
param name:  4.cross_mlp.fc2.bias
param name:  4.cross_gate.weight
param name:  4.cross_gate.bias
param name:  4.cross_adaptive_weights.0.weight
param name:  10.cross_norm.weight
param name:  10.cross_norm.bias
param name:  10.cross_attn.k_proj.base_layer.weight
param name:  10.cross_attn.k_proj.base_layer.bias
param name:  10.cross_attn.k_proj.lora_A.default.weight
param name:  10.cross_attn.k_proj.lora_B.default.weight
param name:  10.cross_attn.v_proj.base_layer.weight
param name:  10.cross_attn.v_proj.base_layer.bias
param name:  10.cross_attn.v_proj.lora_A.default.weight
param name:  10.cross_attn.v_proj.lora_B.default.weight
param name:  10.cross_attn.q_proj.base_layer.weight
param name:  10.cross_attn.q_proj.base_layer.bias
param name:  10.cross_attn.q_proj.lora_A.default.weight
param name:  10.cross_attn.q_proj.lora_B.default.weight
param name:  10.cross_attn.out_proj.base_layer.weight
param name:  10.cross_attn.out_proj.base_layer.bias
param name:  10.cross_attn.out_proj.lora_A.default.weight
param name:  10.cross_attn.out_proj.lora_B.default.weight
param name:  10.cross_mlp.fc1.weight
param name:  10.cross_mlp.fc1.bias
param name:  10.cross_mlp.fc2.weight
param name:  10.cross_mlp.fc2.bias
param name:  10.cross_gate.weight
param name:  10.cross_gate.bias
param name:  10.cross_adaptive_weights.0.weight
param name:  16.cross_norm.weight
param name:  16.cross_norm.bias
param name:  16.cross_attn.k_proj.base_layer.weight
param name:  16.cross_attn.k_proj.base_layer.bias
param name:  16.cross_attn.k_proj.lora_A.default.weight
param name:  16.cross_attn.k_proj.lora_B.default.weight
param name:  16.cross_attn.v_proj.base_layer.weight
param name:  16.cross_attn.v_proj.base_layer.bias
param name:  16.cross_attn.v_proj.lora_A.default.weight
param name:  16.cross_attn.v_proj.lora_B.default.weight
param name:  16.cross_attn.q_proj.base_layer.weight
param name:  16.cross_attn.q_proj.base_layer.bias
param name:  16.cross_attn.q_proj.lora_A.default.weight
param name:  16.cross_attn.q_proj.lora_B.default.weight
param name:  16.cross_attn.out_proj.base_layer.weight
param name:  16.cross_attn.out_proj.base_layer.bias
param name:  16.cross_attn.out_proj.lora_A.default.weight
param name:  16.cross_attn.out_proj.lora_B.default.weight
param name:  16.cross_mlp.fc1.weight
param name:  16.cross_mlp.fc1.bias
param name:  16.cross_mlp.fc2.weight
param name:  16.cross_mlp.fc2.bias
param name:  16.cross_gate.weight
param name:  16.cross_gate.bias
param name:  16.cross_adaptive_weights.0.weight
param name:  22.cross_norm.weight
param name:  22.cross_norm.bias
param name:  22.cross_attn.k_proj.base_layer.weight
param name:  22.cross_attn.k_proj.base_layer.bias
param name:  22.cross_attn.k_proj.lora_A.default.weight
param name:  22.cross_attn.k_proj.lora_B.default.weight
param name:  22.cross_attn.v_proj.base_layer.weight
param name:  22.cross_attn.v_proj.base_layer.bias
param name:  22.cross_attn.v_proj.lora_A.default.weight
param name:  22.cross_attn.v_proj.lora_B.default.weight
param name:  22.cross_attn.q_proj.base_layer.weight
param name:  22.cross_attn.q_proj.base_layer.bias
param name:  22.cross_attn.q_proj.lora_A.default.weight
param name:  22.cross_attn.q_proj.lora_B.default.weight
param name:  22.cross_attn.out_proj.base_layer.weight
param name:  22.cross_attn.out_proj.base_layer.bias
param name:  22.cross_attn.out_proj.lora_A.default.weight
param name:  22.cross_attn.out_proj.lora_B.default.weight
param name:  22.cross_mlp.fc1.weight
param name:  22.cross_mlp.fc1.bias
param name:  22.cross_mlp.fc2.weight
param name:  22.cross_mlp.fc2.bias
param name:  22.cross_gate.weight
param name:  22.cross_gate.bias
param name:  22.cross_adaptive_weights.0.weight
trainable params: 54,811,648 || all params: 491,078,913 || trainable%: 11.1615

normalize_before:  True
number of all params:  543390725
Missing keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias', 'ml_text_feat_perceiver.weight', 'ml_text_feat_perceiver.bias']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.bias']
Current model training epoch is:  54
  0%|          | 0/53 [00:00<?, ?it/s]  0%|          | 0/53 [00:00<?, ?it/s]  2%|         | 1/53 [00:02<01:56,  2.23s/it]  2%|         | 1/53 [00:02<02:00,  2.32s/it]  4%|         | 2/53 [00:02<00:58,  1.15s/it]  4%|         | 2/53 [00:02<01:00,  1.19s/it]  6%|         | 3/53 [00:03<00:40,  1.24it/s]  6%|         | 3/53 [00:03<00:42,  1.16it/s]  8%|         | 4/53 [00:03<00:31,  1.56it/s]  8%|         | 4/53 [00:03<00:33,  1.48it/s]  9%|         | 5/53 [00:03<00:26,  1.80it/s]  9%|         | 5/53 [00:04<00:30,  1.59it/s] 11%|        | 6/53 [00:04<00:23,  2.01it/s] 11%|        | 6/53 [00:04<00:26,  1.75it/s] 13%|        | 7/53 [00:04<00:24,  1.91it/s] 13%|        | 7/53 [00:05<00:24,  1.86it/s] 15%|        | 8/53 [00:05<00:21,  2.08it/s] 17%|        | 9/53 [00:05<00:21,  2.02it/s] 15%|        | 8/53 [00:05<00:29,  1.54it/s] 19%|        | 10/53 [00:06<00:19,  2.17it/s] 17%|        | 9/53 [00:06<00:25,  1.74it/s] 21%|        | 11/53 [00:06<00:20,  2.01it/s] 23%|       | 12/53 [00:07<00:19,  2.15it/s] 19%|        | 10/53 [00:07<00:29,  1.44it/s] 25%|       | 13/53 [00:07<00:19,  2.06it/s] 21%|        | 11/53 [00:07<00:25,  1.66it/s] 26%|       | 14/53 [00:07<00:17,  2.19it/s] 23%|       | 12/53 [00:08<00:24,  1.68it/s] 28%|       | 15/53 [00:08<00:18,  2.01it/s] 25%|       | 13/53 [00:08<00:21,  1.87it/s] 30%|       | 16/53 [00:08<00:17,  2.15it/s] 26%|       | 14/53 [00:09<00:21,  1.84it/s] 32%|      | 17/53 [00:09<00:17,  2.00it/s] 28%|       | 15/53 [00:09<00:18,  2.00it/s] 34%|      | 18/53 [00:09<00:16,  2.15it/s] 30%|       | 16/53 [00:10<00:18,  1.95it/s] 36%|      | 19/53 [00:10<00:17,  1.96it/s] 32%|      | 17/53 [00:10<00:17,  2.09it/s] 38%|      | 20/53 [00:10<00:15,  2.10it/s] 34%|      | 18/53 [00:11<00:16,  2.10it/s] 36%|      | 19/53 [00:11<00:15,  2.21it/s] 40%|      | 21/53 [00:11<00:15,  2.00it/s] 42%|     | 22/53 [00:11<00:14,  2.15it/s] 38%|      | 20/53 [00:12<00:18,  1.74it/s] 43%|     | 23/53 [00:12<00:14,  2.04it/s] 40%|      | 21/53 [00:12<00:16,  1.92it/s] 45%|     | 24/53 [00:12<00:13,  2.17it/s] 42%|     | 22/53 [00:13<00:17,  1.81it/s] 47%|     | 25/53 [00:13<00:14,  2.00it/s] 43%|     | 23/53 [00:13<00:15,  1.98it/s] 49%|     | 26/53 [00:13<00:12,  2.14it/s] 45%|     | 24/53 [00:14<00:14,  1.98it/s] 51%|     | 27/53 [00:14<00:12,  2.05it/s] 47%|     | 25/53 [00:14<00:13,  2.12it/s] 53%|    | 28/53 [00:14<00:11,  2.17it/s] 49%|     | 26/53 [00:15<00:12,  2.16it/s] 55%|    | 29/53 [00:15<00:11,  2.04it/s] 51%|     | 27/53 [00:15<00:11,  2.25it/s] 57%|    | 30/53 [00:15<00:10,  2.18it/s] 53%|    | 28/53 [00:15<00:11,  2.17it/s] 58%|    | 31/53 [00:16<00:10,  2.06it/s] 55%|    | 29/53 [00:16<00:10,  2.24it/s] 60%|    | 32/53 [00:16<00:09,  2.19it/s] 57%|    | 30/53 [00:16<00:10,  2.17it/s] 62%|   | 33/53 [00:17<00:09,  2.11it/s] 64%|   | 34/53 [00:17<00:08,  2.21it/s] 58%|    | 31/53 [00:17<00:11,  1.86it/s] 60%|    | 32/53 [00:18<00:10,  2.01it/s] 66%|   | 35/53 [00:18<00:08,  2.02it/s] 68%|   | 36/53 [00:18<00:07,  2.16it/s] 62%|   | 33/53 [00:18<00:10,  1.92it/s] 64%|   | 34/53 [00:18<00:09,  2.06it/s] 70%|   | 37/53 [00:19<00:07,  2.08it/s] 72%|  | 38/53 [00:19<00:07,  2.08it/s] 66%|   | 35/53 [00:19<00:08,  2.00it/s] 68%|   | 36/53 [00:19<00:08,  2.12it/s] 74%|  | 39/53 [00:20<00:06,  2.03it/s] 70%|   | 37/53 [00:20<00:07,  2.03it/s] 75%|  | 40/53 [00:20<00:06,  2.07it/s] 72%|  | 38/53 [00:20<00:07,  2.14it/s] 77%|  | 41/53 [00:20<00:05,  2.06it/s] 74%|  | 39/53 [00:21<00:06,  2.04it/s] 79%|  | 42/53 [00:21<00:05,  2.09it/s] 75%|  | 40/53 [00:21<00:06,  2.16it/s] 81%|  | 43/53 [00:21<00:04,  2.08it/s] 77%|  | 41/53 [00:22<00:05,  2.09it/s] 83%| | 44/53 [00:22<00:04,  2.04it/s] 79%|  | 42/53 [00:22<00:04,  2.20it/s] 85%| | 45/53 [00:22<00:03,  2.11it/s] 87%| | 46/53 [00:23<00:03,  2.04it/s] 81%|  | 43/53 [00:23<00:05,  1.73it/s] 89%| | 47/53 [00:23<00:02,  2.07it/s] 83%| | 44/53 [00:23<00:04,  1.90it/s] 91%| | 48/53 [00:24<00:02,  1.98it/s] 85%| | 45/53 [00:24<00:04,  1.70it/s] 92%|| 49/53 [00:24<00:01,  2.12it/s] 87%| | 46/53 [00:25<00:03,  1.88it/s] 94%|| 50/53 [00:25<00:01,  2.05it/s] 89%| | 47/53 [00:25<00:03,  1.93it/s] 96%|| 51/53 [00:25<00:00,  2.17it/s] 91%| | 48/53 [00:26<00:02,  2.08it/s] 98%|| 52/53 [00:26<00:00,  1.98it/s] 92%|| 49/53 [00:26<00:01,  2.06it/s]100%|| 53/53 [00:26<00:00,  2.42it/s]100%|| 53/53 [00:26<00:00,  1.99it/s]
 94%|| 50/53 [00:26<00:01,  2.15it/s] 96%|| 51/53 [00:27<00:00,  2.10it/s] 98%|| 52/53 [00:27<00:00,  2.20it/s]100%|| 53/53 [00:28<00:00,  2.64it/s]100%|| 53/53 [00:28<00:00,  1.88it/s]
accuracy2:  0.2889300847457627
Testing time 0:00:32
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_large/best_checkpoint.pth', 'test_set_accuracy': 0.2889300847457627}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:20:23,813] torch.distributed.run: [WARNING] 
[2025-11-11 09:20:23,813] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:20:23,813] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:20:23,813] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 1): env://
| distributed init (rank 0): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-L/14
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.

extract vision layer:  [6, 12, 18, 24]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [4, 10, 16, 22]
trainable params: 9,699,328 || all params: 491,078,913 || trainable%: 1.9751
trainable params: 0 || all params: 491,078,913 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  4.cross_norm.weight
param name:  4.cross_norm.bias
param name:  4.cross_attn.k_proj.base_layer.weight
param name:  4.cross_attn.k_proj.base_layer.bias
param name:  4.cross_attn.k_proj.lora_A.default.weight
param name:  4.cross_attn.k_proj.lora_B.default.weight
param name:  4.cross_attn.v_proj.base_layer.weight
param name:  4.cross_attn.v_proj.base_layer.bias
param name:  4.cross_attn.v_proj.lora_A.default.weight
param name:  4.cross_attn.v_proj.lora_B.default.weight
param name:  4.cross_attn.q_proj.base_layer.weight
param name:  4.cross_attn.q_proj.base_layer.bias
param name:  4.cross_attn.q_proj.lora_A.default.weight
param name:  4.cross_attn.q_proj.lora_B.default.weight
param name:  4.cross_attn.out_proj.base_layer.weight
param name:  4.cross_attn.out_proj.base_layer.bias
param name:  4.cross_attn.out_proj.lora_A.default.weight
param name:  4.cross_attn.out_proj.lora_B.default.weight
param name:  4.cross_mlp.fc1.weight
param name:  4.cross_mlp.fc1.bias
param name:  4.cross_mlp.fc2.weight
param name:  4.cross_mlp.fc2.bias
param name:  4.cross_gate.weight
param name:  4.cross_gate.bias
param name:  4.cross_adaptive_weights.0.weight
param name:  10.cross_norm.weight
param name:  10.cross_norm.bias
param name:  10.cross_attn.k_proj.base_layer.weight
param name:  10.cross_attn.k_proj.base_layer.bias
param name:  10.cross_attn.k_proj.lora_A.default.weight
param name:  10.cross_attn.k_proj.lora_B.default.weight
param name:  10.cross_attn.v_proj.base_layer.weight
param name:  10.cross_attn.v_proj.base_layer.bias
param name:  10.cross_attn.v_proj.lora_A.default.weight
param name:  10.cross_attn.v_proj.lora_B.default.weight
param name:  10.cross_attn.q_proj.base_layer.weight
param name:  10.cross_attn.q_proj.base_layer.bias
param name:  10.cross_attn.q_proj.lora_A.default.weight
param name:  10.cross_attn.q_proj.lora_B.default.weight
param name:  10.cross_attn.out_proj.base_layer.weight
param name:  10.cross_attn.out_proj.base_layer.bias
param name:  10.cross_attn.out_proj.lora_A.default.weight
param name:  10.cross_attn.out_proj.lora_B.default.weight
param name:  10.cross_mlp.fc1.weight
param name:  10.cross_mlp.fc1.bias
param name:  10.cross_mlp.fc2.weight
param name:  10.cross_mlp.fc2.bias
param name:  10.cross_gate.weight
param name:  10.cross_gate.bias
param name:  10.cross_adaptive_weights.0.weight
param name:  16.cross_norm.weight
param name:  16.cross_norm.bias
param name:  16.cross_attn.k_proj.base_layer.weight
param name:  16.cross_attn.k_proj.base_layer.bias
param name:  16.cross_attn.k_proj.lora_A.default.weight
param name:  16.cross_attn.k_proj.lora_B.default.weight
param name:  16.cross_attn.v_proj.base_layer.weight
param name:  16.cross_attn.v_proj.base_layer.bias
param name:  16.cross_attn.v_proj.lora_A.default.weight
param name:  16.cross_attn.v_proj.lora_B.default.weight
param name:  16.cross_attn.q_proj.base_layer.weight
param name:  16.cross_attn.q_proj.base_layer.bias
param name:  16.cross_attn.q_proj.lora_A.default.weight
param name:  16.cross_attn.q_proj.lora_B.default.weight
param name:  16.cross_attn.out_proj.base_layer.weight
param name:  16.cross_attn.out_proj.base_layer.bias
param name:  16.cross_attn.out_proj.lora_A.default.weight
param name:  16.cross_attn.out_proj.lora_B.default.weight
param name:  16.cross_mlp.fc1.weight
param name:  16.cross_mlp.fc1.bias
param name:  16.cross_mlp.fc2.weight
param name:  16.cross_mlp.fc2.bias
param name:  16.cross_gate.weight
param name:  16.cross_gate.bias
param name:  16.cross_adaptive_weights.0.weight
param name:  22.cross_norm.weight
param name:  22.cross_norm.bias
param name:  22.cross_attn.k_proj.base_layer.weight
param name:  22.cross_attn.k_proj.base_layer.bias
param name:  22.cross_attn.k_proj.lora_A.default.weight
param name:  22.cross_attn.k_proj.lora_B.default.weight
param name:  22.cross_attn.v_proj.base_layer.weight
param name:  22.cross_attn.v_proj.base_layer.bias
param name:  22.cross_attn.v_proj.lora_A.default.weight
param name:  22.cross_attn.v_proj.lora_B.default.weight
param name:  22.cross_attn.q_proj.base_layer.weight
param name:  22.cross_attn.q_proj.base_layer.bias
param name:  22.cross_attn.q_proj.lora_A.default.weight
param name:  22.cross_attn.q_proj.lora_B.default.weight
param name:  22.cross_attn.out_proj.base_layer.weight
param name:  22.cross_attn.out_proj.base_layer.bias
param name:  22.cross_attn.out_proj.lora_A.default.weight
param name:  22.cross_attn.out_proj.lora_B.default.weight
param name:  22.cross_mlp.fc1.weight
param name:  22.cross_mlp.fc1.bias
param name:  22.cross_mlp.fc2.weight
param name:  22.cross_mlp.fc2.bias
param name:  22.cross_gate.weight
param name:  22.cross_gate.bias
param name:  22.cross_adaptive_weights.0.weight
trainable params: 54,811,648 || all params: 491,078,913 || trainable%: 11.1615

normalize_before:  True
number of all params:  543390725
Missing keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias', 'ml_text_feat_perceiver.weight', 'ml_text_feat_perceiver.bias']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.bias']
Current model training epoch is:  54
  0%|          | 0/18 [00:00<?, ?it/s]  0%|          | 0/18 [00:00<?, ?it/s]  6%|         | 1/18 [00:02<00:41,  2.44s/it]  6%|         | 1/18 [00:02<00:42,  2.49s/it] 11%|         | 2/18 [00:02<00:19,  1.23s/it] 11%|         | 2/18 [00:02<00:20,  1.26s/it] 17%|        | 3/18 [00:03<00:12,  1.18it/s] 17%|        | 3/18 [00:03<00:13,  1.15it/s] 22%|       | 4/18 [00:03<00:09,  1.50it/s] 22%|       | 4/18 [00:03<00:09,  1.47it/s] 28%|       | 5/18 [00:04<00:07,  1.73it/s] 28%|       | 5/18 [00:04<00:08,  1.57it/s] 33%|      | 6/18 [00:04<00:06,  1.80it/s] 33%|      | 6/18 [00:04<00:06,  1.84it/s] 39%|      | 7/18 [00:04<00:05,  2.02it/s] 39%|      | 7/18 [00:05<00:06,  1.73it/s] 44%|     | 8/18 [00:05<00:05,  1.95it/s] 44%|     | 8/18 [00:05<00:05,  1.93it/s] 50%|     | 9/18 [00:05<00:04,  2.08it/s] 50%|     | 9/18 [00:06<00:04,  1.85it/s] 56%|    | 10/18 [00:06<00:04,  1.93it/s] 56%|    | 10/18 [00:06<00:03,  2.02it/s] 61%|    | 11/18 [00:06<00:03,  2.09it/s] 61%|    | 11/18 [00:07<00:03,  1.92it/s] 67%|   | 12/18 [00:07<00:02,  2.09it/s] 67%|   | 12/18 [00:07<00:03,  1.92it/s] 72%|  | 13/18 [00:07<00:02,  2.06it/s] 72%|  | 13/18 [00:08<00:02,  1.95it/s] 78%|  | 14/18 [00:08<00:01,  2.11it/s] 78%|  | 14/18 [00:08<00:02,  1.86it/s] 83%| | 15/18 [00:08<00:01,  2.01it/s] 83%| | 15/18 [00:09<00:01,  1.94it/s] 89%| | 16/18 [00:09<00:00,  2.11it/s] 89%| | 16/18 [00:09<00:01,  1.97it/s] 94%|| 17/18 [00:09<00:00,  2.11it/s]100%|| 18/18 [00:10<00:00,  2.75it/s] 94%|| 17/18 [00:10<00:00,  2.00it/s]100%|| 18/18 [00:10<00:00,  1.77it/s]
100%|| 18/18 [00:10<00:00,  1.75it/s]
accuracy2:  0.5535714285714286
Testing time 0:00:11
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_large/best_checkpoint.pth', 'testA_set_accuracy': 0.5535714285714286}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:21:01,042] torch.distributed.run: [WARNING] 
[2025-11-11 09:21:01,042] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:21:01,042] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:21:01,042] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-L/14
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.

extract vision layer:  [6, 12, 18, 24]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [4, 10, 16, 22]
trainable params: 9,699,328 || all params: 491,078,913 || trainable%: 1.9751
trainable params: 0 || all params: 491,078,913 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  4.cross_norm.weight
param name:  4.cross_norm.bias
param name:  4.cross_attn.k_proj.base_layer.weight
param name:  4.cross_attn.k_proj.base_layer.bias
param name:  4.cross_attn.k_proj.lora_A.default.weight
param name:  4.cross_attn.k_proj.lora_B.default.weight
param name:  4.cross_attn.v_proj.base_layer.weight
param name:  4.cross_attn.v_proj.base_layer.bias
param name:  4.cross_attn.v_proj.lora_A.default.weight
param name:  4.cross_attn.v_proj.lora_B.default.weight
param name:  4.cross_attn.q_proj.base_layer.weight
param name:  4.cross_attn.q_proj.base_layer.bias
param name:  4.cross_attn.q_proj.lora_A.default.weight
param name:  4.cross_attn.q_proj.lora_B.default.weight
param name:  4.cross_attn.out_proj.base_layer.weight
param name:  4.cross_attn.out_proj.base_layer.bias
param name:  4.cross_attn.out_proj.lora_A.default.weight
param name:  4.cross_attn.out_proj.lora_B.default.weight
param name:  4.cross_mlp.fc1.weight
param name:  4.cross_mlp.fc1.bias
param name:  4.cross_mlp.fc2.weight
param name:  4.cross_mlp.fc2.bias
param name:  4.cross_gate.weight
param name:  4.cross_gate.bias
param name:  4.cross_adaptive_weights.0.weight
param name:  10.cross_norm.weight
param name:  10.cross_norm.bias
param name:  10.cross_attn.k_proj.base_layer.weight
param name:  10.cross_attn.k_proj.base_layer.bias
param name:  10.cross_attn.k_proj.lora_A.default.weight
param name:  10.cross_attn.k_proj.lora_B.default.weight
param name:  10.cross_attn.v_proj.base_layer.weight
param name:  10.cross_attn.v_proj.base_layer.bias
param name:  10.cross_attn.v_proj.lora_A.default.weight
param name:  10.cross_attn.v_proj.lora_B.default.weight
param name:  10.cross_attn.q_proj.base_layer.weight
param name:  10.cross_attn.q_proj.base_layer.bias
param name:  10.cross_attn.q_proj.lora_A.default.weight
param name:  10.cross_attn.q_proj.lora_B.default.weight
param name:  10.cross_attn.out_proj.base_layer.weight
param name:  10.cross_attn.out_proj.base_layer.bias
param name:  10.cross_attn.out_proj.lora_A.default.weight
param name:  10.cross_attn.out_proj.lora_B.default.weight
param name:  10.cross_mlp.fc1.weight
param name:  10.cross_mlp.fc1.bias
param name:  10.cross_mlp.fc2.weight
param name:  10.cross_mlp.fc2.bias
param name:  10.cross_gate.weight
param name:  10.cross_gate.bias
param name:  10.cross_adaptive_weights.0.weight
param name:  16.cross_norm.weight
param name:  16.cross_norm.bias
param name:  16.cross_attn.k_proj.base_layer.weight
param name:  16.cross_attn.k_proj.base_layer.bias
param name:  16.cross_attn.k_proj.lora_A.default.weight
param name:  16.cross_attn.k_proj.lora_B.default.weight
param name:  16.cross_attn.v_proj.base_layer.weight
param name:  16.cross_attn.v_proj.base_layer.bias
param name:  16.cross_attn.v_proj.lora_A.default.weight
param name:  16.cross_attn.v_proj.lora_B.default.weight
param name:  16.cross_attn.q_proj.base_layer.weight
param name:  16.cross_attn.q_proj.base_layer.bias
param name:  16.cross_attn.q_proj.lora_A.default.weight
param name:  16.cross_attn.q_proj.lora_B.default.weight
param name:  16.cross_attn.out_proj.base_layer.weight
param name:  16.cross_attn.out_proj.base_layer.bias
param name:  16.cross_attn.out_proj.lora_A.default.weight
param name:  16.cross_attn.out_proj.lora_B.default.weight
param name:  16.cross_mlp.fc1.weight
param name:  16.cross_mlp.fc1.bias
param name:  16.cross_mlp.fc2.weight
param name:  16.cross_mlp.fc2.bias
param name:  16.cross_gate.weight
param name:  16.cross_gate.bias
param name:  16.cross_adaptive_weights.0.weight
param name:  22.cross_norm.weight
param name:  22.cross_norm.bias
param name:  22.cross_attn.k_proj.base_layer.weight
param name:  22.cross_attn.k_proj.base_layer.bias
param name:  22.cross_attn.k_proj.lora_A.default.weight
param name:  22.cross_attn.k_proj.lora_B.default.weight
param name:  22.cross_attn.v_proj.base_layer.weight
param name:  22.cross_attn.v_proj.base_layer.bias
param name:  22.cross_attn.v_proj.lora_A.default.weight
param name:  22.cross_attn.v_proj.lora_B.default.weight
param name:  22.cross_attn.q_proj.base_layer.weight
param name:  22.cross_attn.q_proj.base_layer.bias
param name:  22.cross_attn.q_proj.lora_A.default.weight
param name:  22.cross_attn.q_proj.lora_B.default.weight
param name:  22.cross_attn.out_proj.base_layer.weight
param name:  22.cross_attn.out_proj.base_layer.bias
param name:  22.cross_attn.out_proj.lora_A.default.weight
param name:  22.cross_attn.out_proj.lora_B.default.weight
param name:  22.cross_mlp.fc1.weight
param name:  22.cross_mlp.fc1.bias
param name:  22.cross_mlp.fc2.weight
param name:  22.cross_mlp.fc2.bias
param name:  22.cross_gate.weight
param name:  22.cross_gate.bias
param name:  22.cross_adaptive_weights.0.weight
trainable params: 54,811,648 || all params: 491,078,913 || trainable%: 11.1615

normalize_before:  True
number of all params:  543390725
  0%|          | 0/16 [00:00<?, ?it/s]Missing keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias', 'ml_text_feat_perceiver.weight', 'ml_text_feat_perceiver.bias']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.bias']
Current model training epoch is:  54
  0%|          | 0/16 [00:00<?, ?it/s]  6%|         | 1/16 [00:02<00:35,  2.37s/it]  6%|         | 1/16 [00:02<00:35,  2.36s/it] 12%|        | 2/16 [00:02<00:17,  1.22s/it] 12%|        | 2/16 [00:02<00:16,  1.21s/it] 19%|        | 3/16 [00:03<00:11,  1.18it/s] 19%|        | 3/16 [00:03<00:10,  1.19it/s] 25%|       | 4/16 [00:03<00:08,  1.49it/s] 25%|       | 4/16 [00:03<00:07,  1.50it/s] 31%|      | 5/16 [00:03<00:06,  1.74it/s] 38%|      | 6/16 [00:04<00:05,  1.94it/s] 31%|      | 5/16 [00:04<00:08,  1.23it/s] 44%|     | 7/16 [00:04<00:04,  2.10it/s] 38%|      | 6/16 [00:05<00:06,  1.50it/s] 50%|     | 8/16 [00:05<00:03,  2.20it/s] 56%|    | 9/16 [00:05<00:03,  2.20it/s] 44%|     | 7/16 [00:05<00:05,  1.55it/s] 62%|   | 10/16 [00:06<00:02,  2.28it/s] 50%|     | 8/16 [00:06<00:04,  1.77it/s] 56%|    | 9/16 [00:06<00:04,  1.69it/s] 69%|   | 11/16 [00:06<00:02,  1.97it/s] 62%|   | 10/16 [00:07<00:03,  1.88it/s] 75%|  | 12/16 [00:07<00:01,  2.10it/s] 69%|   | 11/16 [00:07<00:02,  1.84it/s] 81%| | 13/16 [00:07<00:01,  1.96it/s] 75%|  | 12/16 [00:08<00:02,  1.99it/s] 88%| | 14/16 [00:08<00:00,  2.10it/s] 81%| | 13/16 [00:08<00:01,  1.87it/s] 94%|| 15/16 [00:08<00:00,  1.85it/s]100%|| 16/16 [00:08<00:00,  2.44it/s]100%|| 16/16 [00:09<00:00,  1.76it/s]
 88%| | 14/16 [00:09<00:00,  2.03it/s] 94%|| 15/16 [00:09<00:00,  1.98it/s]100%|| 16/16 [00:09<00:00,  2.58it/s]100%|| 16/16 [00:09<00:00,  1.63it/s]
accuracy2:  0.3647166361974406
Testing time 0:00:11
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_large/best_checkpoint.pth', 'testB_set_accuracy': 0.3647166361974406}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:21:38,572] torch.distributed.run: [WARNING] 
[2025-11-11 09:21:38,572] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:21:38,572] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:21:38,572] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-L/14
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.

extract vision layer:  [6, 12, 18, 24]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [4, 10, 16, 22]
trainable params: 9,699,328 || all params: 491,078,913 || trainable%: 1.9751
trainable params: 0 || all params: 491,078,913 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  4.cross_norm.weight
param name:  4.cross_norm.bias
param name:  4.cross_attn.k_proj.base_layer.weight
param name:  4.cross_attn.k_proj.base_layer.bias
param name:  4.cross_attn.k_proj.lora_A.default.weight
param name:  4.cross_attn.k_proj.lora_B.default.weight
param name:  4.cross_attn.v_proj.base_layer.weight
param name:  4.cross_attn.v_proj.base_layer.bias
param name:  4.cross_attn.v_proj.lora_A.default.weight
param name:  4.cross_attn.v_proj.lora_B.default.weight
param name:  4.cross_attn.q_proj.base_layer.weight
param name:  4.cross_attn.q_proj.base_layer.bias
param name:  4.cross_attn.q_proj.lora_A.default.weight
param name:  4.cross_attn.q_proj.lora_B.default.weight
param name:  4.cross_attn.out_proj.base_layer.weight
param name:  4.cross_attn.out_proj.base_layer.bias
param name:  4.cross_attn.out_proj.lora_A.default.weight
param name:  4.cross_attn.out_proj.lora_B.default.weight
param name:  4.cross_mlp.fc1.weight
param name:  4.cross_mlp.fc1.bias
param name:  4.cross_mlp.fc2.weight
param name:  4.cross_mlp.fc2.bias
param name:  4.cross_gate.weight
param name:  4.cross_gate.bias
param name:  4.cross_adaptive_weights.0.weight
param name:  10.cross_norm.weight
param name:  10.cross_norm.bias
param name:  10.cross_attn.k_proj.base_layer.weight
param name:  10.cross_attn.k_proj.base_layer.bias
param name:  10.cross_attn.k_proj.lora_A.default.weight
param name:  10.cross_attn.k_proj.lora_B.default.weight
param name:  10.cross_attn.v_proj.base_layer.weight
param name:  10.cross_attn.v_proj.base_layer.bias
param name:  10.cross_attn.v_proj.lora_A.default.weight
param name:  10.cross_attn.v_proj.lora_B.default.weight
param name:  10.cross_attn.q_proj.base_layer.weight
param name:  10.cross_attn.q_proj.base_layer.bias
param name:  10.cross_attn.q_proj.lora_A.default.weight
param name:  10.cross_attn.q_proj.lora_B.default.weight
param name:  10.cross_attn.out_proj.base_layer.weight
param name:  10.cross_attn.out_proj.base_layer.bias
param name:  10.cross_attn.out_proj.lora_A.default.weight
param name:  10.cross_attn.out_proj.lora_B.default.weight
param name:  10.cross_mlp.fc1.weight
param name:  10.cross_mlp.fc1.bias
param name:  10.cross_mlp.fc2.weight
param name:  10.cross_mlp.fc2.bias
param name:  10.cross_gate.weight
param name:  10.cross_gate.bias
param name:  10.cross_adaptive_weights.0.weight
param name:  16.cross_norm.weight
param name:  16.cross_norm.bias
param name:  16.cross_attn.k_proj.base_layer.weight
param name:  16.cross_attn.k_proj.base_layer.bias
param name:  16.cross_attn.k_proj.lora_A.default.weight
param name:  16.cross_attn.k_proj.lora_B.default.weight
param name:  16.cross_attn.v_proj.base_layer.weight
param name:  16.cross_attn.v_proj.base_layer.bias
param name:  16.cross_attn.v_proj.lora_A.default.weight
param name:  16.cross_attn.v_proj.lora_B.default.weight
param name:  16.cross_attn.q_proj.base_layer.weight
param name:  16.cross_attn.q_proj.base_layer.bias
param name:  16.cross_attn.q_proj.lora_A.default.weight
param name:  16.cross_attn.q_proj.lora_B.default.weight
param name:  16.cross_attn.out_proj.base_layer.weight
param name:  16.cross_attn.out_proj.base_layer.bias
param name:  16.cross_attn.out_proj.lora_A.default.weight
param name:  16.cross_attn.out_proj.lora_B.default.weight
param name:  16.cross_mlp.fc1.weight
param name:  16.cross_mlp.fc1.bias
param name:  16.cross_mlp.fc2.weight
param name:  16.cross_mlp.fc2.bias
param name:  16.cross_gate.weight
param name:  16.cross_gate.bias
param name:  16.cross_adaptive_weights.0.weight
param name:  22.cross_norm.weight
param name:  22.cross_norm.bias
param name:  22.cross_attn.k_proj.base_layer.weight
param name:  22.cross_attn.k_proj.base_layer.bias
param name:  22.cross_attn.k_proj.lora_A.default.weight
param name:  22.cross_attn.k_proj.lora_B.default.weight
param name:  22.cross_attn.v_proj.base_layer.weight
param name:  22.cross_attn.v_proj.base_layer.bias
param name:  22.cross_attn.v_proj.lora_A.default.weight
param name:  22.cross_attn.v_proj.lora_B.default.weight
param name:  22.cross_attn.q_proj.base_layer.weight
param name:  22.cross_attn.q_proj.base_layer.bias
param name:  22.cross_attn.q_proj.lora_A.default.weight
param name:  22.cross_attn.q_proj.lora_B.default.weight
param name:  22.cross_attn.out_proj.base_layer.weight
param name:  22.cross_attn.out_proj.base_layer.bias
param name:  22.cross_attn.out_proj.lora_A.default.weight
param name:  22.cross_attn.out_proj.lora_B.default.weight
param name:  22.cross_mlp.fc1.weight
param name:  22.cross_mlp.fc1.bias
param name:  22.cross_mlp.fc2.weight
param name:  22.cross_mlp.fc2.bias
param name:  22.cross_gate.weight
param name:  22.cross_gate.bias
param name:  22.cross_adaptive_weights.0.weight
trainable params: 54,811,648 || all params: 491,078,913 || trainable%: 11.1615

normalize_before:  True
number of all params:  543390725
Missing keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias', 'ml_text_feat_perceiver.weight', 'ml_text_feat_perceiver.bias']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.4.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.10.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.16.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.22.cross_attn.out_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'clip.base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.bias']
Current model training epoch is:  54
  0%|          | 0/26 [00:00<?, ?it/s]  0%|          | 0/26 [00:00<?, ?it/s]  4%|         | 1/26 [00:02<01:06,  2.64s/it]  4%|         | 1/26 [00:02<01:06,  2.68s/it]  8%|         | 2/26 [00:03<00:31,  1.32s/it]  8%|         | 2/26 [00:03<00:32,  1.34s/it] 12%|        | 3/26 [00:03<00:20,  1.11it/s] 12%|        | 3/26 [00:03<00:21,  1.08it/s] 15%|        | 4/26 [00:03<00:15,  1.43it/s] 15%|        | 4/26 [00:03<00:15,  1.39it/s] 19%|        | 5/26 [00:04<00:13,  1.53it/s] 23%|       | 6/26 [00:04<00:11,  1.78it/s] 19%|        | 5/26 [00:04<00:17,  1.20it/s] 23%|       | 6/26 [00:05<00:13,  1.46it/s] 27%|       | 7/26 [00:05<00:11,  1.66it/s] 31%|       | 8/26 [00:05<00:09,  1.86it/s] 27%|       | 7/26 [00:06<00:14,  1.27it/s] 35%|      | 9/26 [00:06<00:09,  1.78it/s] 31%|       | 8/26 [00:06<00:11,  1.51it/s] 38%|      | 10/26 [00:06<00:08,  1.96it/s] 35%|      | 9/26 [00:07<00:12,  1.34it/s] 42%|     | 11/26 [00:07<00:10,  1.46it/s] 38%|      | 10/26 [00:08<00:10,  1.57it/s] 46%|     | 12/26 [00:08<00:08,  1.67it/s] 42%|     | 11/26 [00:09<00:10,  1.37it/s] 50%|     | 13/26 [00:09<00:09,  1.37it/s] 46%|     | 12/26 [00:09<00:08,  1.59it/s] 54%|    | 14/26 [00:09<00:07,  1.58it/s] 50%|     | 13/26 [00:10<00:08,  1.58it/s] 54%|    | 14/26 [00:10<00:06,  1.78it/s] 58%|    | 15/26 [00:10<00:08,  1.34it/s] 58%|    | 15/26 [00:10<00:06,  1.79it/s] 62%|   | 16/26 [00:11<00:06,  1.56it/s] 62%|   | 16/26 [00:11<00:05,  1.94it/s] 65%|   | 17/26 [00:11<00:04,  1.95it/s] 65%|   | 17/26 [00:12<00:06,  1.38it/s] 69%|   | 18/26 [00:12<00:03,  2.10it/s] 69%|   | 18/26 [00:12<00:05,  1.59it/s] 73%|  | 19/26 [00:12<00:03,  1.96it/s] 73%|  | 19/26 [00:13<00:05,  1.37it/s] 77%|  | 20/26 [00:13<00:03,  1.79it/s] 77%|  | 20/26 [00:13<00:03,  1.59it/s] 81%|  | 21/26 [00:13<00:02,  1.94it/s] 81%|  | 21/26 [00:14<00:03,  1.57it/s] 85%| | 22/26 [00:14<00:02,  1.77it/s] 85%| | 22/26 [00:15<00:02,  1.45it/s] 88%| | 23/26 [00:15<00:01,  1.66it/s] 88%| | 23/26 [00:15<00:01,  1.72it/s] 92%|| 24/26 [00:15<00:01,  1.91it/s] 92%|| 24/26 [00:16<00:01,  1.36it/s] 96%|| 25/26 [00:16<00:00,  1.79it/s]100%|| 26/26 [00:16<00:00,  2.11it/s] 96%|| 25/26 [00:16<00:00,  1.58it/s]100%|| 26/26 [00:17<00:00,  1.53it/s]
100%|| 26/26 [00:17<00:00,  1.90it/s]100%|| 26/26 [00:17<00:00,  1.49it/s]
accuracy2:  0.02922077922077922
Testing time 0:00:19
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_large/best_checkpoint.pth', 'testC_set_accuracy': 0.02922077922077922}
