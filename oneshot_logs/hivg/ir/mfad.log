/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:15:48,866] torch.distributed.run: [WARNING] 
[2025-11-11 09:15:48,866] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:15:48,866] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:15:48,866] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 1): env://
| distributed init (rank 0): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
   0%|          | 0/18 [00:00<?, ?it/s]['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/18 [00:00<?, ?it/s]  6%|         | 1/18 [00:01<00:33,  1.98s/it] 11%|         | 2/18 [00:02<00:15,  1.02it/s]  6%|         | 1/18 [00:02<00:39,  2.34s/it] 17%|        | 3/18 [00:02<00:09,  1.51it/s] 11%|         | 2/18 [00:02<00:18,  1.15s/it] 22%|       | 4/18 [00:02<00:06,  2.02it/s] 17%|        | 3/18 [00:02<00:11,  1.31it/s] 22%|       | 4/18 [00:03<00:07,  1.80it/s] 28%|       | 5/18 [00:03<00:06,  1.88it/s] 33%|      | 6/18 [00:03<00:04,  2.50it/s] 28%|       | 5/18 [00:04<00:08,  1.54it/s] 39%|      | 7/18 [00:04<00:04,  2.20it/s] 33%|      | 6/18 [00:04<00:05,  2.07it/s] 44%|     | 8/18 [00:04<00:03,  2.82it/s] 50%|     | 9/18 [00:04<00:03,  2.27it/s] 39%|      | 7/18 [00:04<00:06,  1.72it/s] 56%|    | 10/18 [00:05<00:02,  2.87it/s] 44%|     | 8/18 [00:05<00:04,  2.22it/s] 61%|    | 11/18 [00:05<00:03,  2.12it/s] 67%|   | 12/18 [00:05<00:02,  2.70it/s] 50%|     | 9/18 [00:05<00:05,  1.78it/s] 56%|    | 10/18 [00:06<00:03,  2.09it/s] 72%|  | 13/18 [00:06<00:02,  2.26it/s] 78%|  | 14/18 [00:06<00:01,  2.84it/s] 61%|    | 11/18 [00:06<00:03,  2.14it/s] 67%|   | 12/18 [00:06<00:02,  2.66it/s] 83%| | 15/18 [00:07<00:01,  2.26it/s] 89%| | 16/18 [00:07<00:00,  2.83it/s] 72%|  | 13/18 [00:07<00:02,  2.14it/s] 78%|  | 14/18 [00:07<00:01,  2.67it/s] 94%|| 17/18 [00:08<00:00,  2.32it/s]100%|| 18/18 [00:08<00:00,  2.19it/s]
 83%| | 15/18 [00:08<00:01,  2.13it/s] 89%| | 16/18 [00:08<00:00,  2.68it/s] 94%|| 17/18 [00:09<00:00,  2.09it/s]100%|| 18/18 [00:09<00:00,  1.91it/s]
accuracy2:  0.06289808917197452
Testing time 0:00:10
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'val_set_accuracy': 0.06289808917197452}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:16:21,449] torch.distributed.run: [WARNING] 
[2025-11-11 09:16:21,449] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:16:21,449] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:16:21,449] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
  0%|          | 0/60 [00:00<?, ?it/s]Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/60 [00:00<?, ?it/s]  2%|         | 1/60 [00:02<02:02,  2.07s/it]  2%|         | 1/60 [00:02<02:05,  2.12s/it]  3%|         | 2/60 [00:02<01:00,  1.04s/it]  3%|         | 2/60 [00:02<01:01,  1.06s/it]  5%|         | 3/60 [00:02<00:40,  1.42it/s]  5%|         | 3/60 [00:02<00:39,  1.45it/s]  7%|         | 4/60 [00:03<00:30,  1.82it/s]  7%|         | 4/60 [00:03<00:31,  1.79it/s]  8%|         | 5/60 [00:03<00:26,  2.11it/s]  8%|         | 5/60 [00:04<00:39,  1.41it/s] 10%|         | 6/60 [00:04<00:31,  1.70it/s] 10%|         | 6/60 [00:04<00:29,  1.81it/s] 12%|        | 7/60 [00:04<00:25,  2.05it/s] 12%|        | 7/60 [00:05<00:33,  1.57it/s] 13%|        | 8/60 [00:05<00:28,  1.81it/s] 13%|        | 8/60 [00:05<00:27,  1.89it/s] 15%|        | 9/60 [00:05<00:23,  2.20it/s] 15%|        | 9/60 [00:05<00:26,  1.89it/s] 17%|        | 10/60 [00:05<00:24,  2.01it/s] 17%|        | 10/60 [00:06<00:20,  2.39it/s] 18%|        | 11/60 [00:06<00:19,  2.49it/s] 20%|        | 12/60 [00:06<00:23,  2.07it/s] 18%|        | 11/60 [00:06<00:26,  1.83it/s] 22%|       | 13/60 [00:06<00:18,  2.60it/s] 20%|        | 12/60 [00:07<00:20,  2.32it/s] 22%|       | 13/60 [00:07<00:23,  1.99it/s] 23%|       | 14/60 [00:07<00:22,  2.03it/s] 23%|       | 14/60 [00:07<00:18,  2.47it/s] 25%|       | 15/60 [00:07<00:17,  2.51it/s] 27%|       | 16/60 [00:08<00:21,  2.00it/s] 25%|       | 15/60 [00:08<00:23,  1.93it/s] 27%|       | 16/60 [00:08<00:18,  2.43it/s] 28%|       | 17/60 [00:08<00:17,  2.44it/s] 30%|       | 18/60 [00:09<00:21,  1.99it/s] 28%|       | 17/60 [00:09<00:22,  1.92it/s] 32%|      | 19/60 [00:09<00:16,  2.48it/s] 30%|       | 18/60 [00:09<00:17,  2.40it/s] 33%|      | 20/60 [00:10<00:19,  2.04it/s] 32%|      | 19/60 [00:10<00:20,  2.03it/s] 33%|      | 20/60 [00:10<00:15,  2.52it/s] 35%|      | 21/60 [00:10<00:16,  2.41it/s] 35%|      | 21/60 [00:11<00:19,  2.02it/s] 37%|      | 22/60 [00:11<00:18,  2.04it/s] 37%|      | 22/60 [00:11<00:15,  2.53it/s] 38%|      | 23/60 [00:11<00:14,  2.51it/s] 38%|      | 23/60 [00:12<00:18,  2.02it/s] 40%|      | 24/60 [00:12<00:17,  2.05it/s] 40%|      | 24/60 [00:12<00:14,  2.51it/s] 42%|     | 25/60 [00:12<00:14,  2.46it/s] 42%|     | 25/60 [00:13<00:17,  2.03it/s] 43%|     | 26/60 [00:13<00:16,  2.01it/s] 43%|     | 26/60 [00:13<00:13,  2.54it/s] 45%|     | 27/60 [00:13<00:13,  2.43it/s] 45%|     | 27/60 [00:14<00:16,  1.98it/s] 47%|     | 28/60 [00:14<00:15,  2.03it/s] 47%|     | 28/60 [00:14<00:12,  2.47it/s] 48%|     | 29/60 [00:14<00:12,  2.50it/s] 50%|     | 30/60 [00:14<00:14,  2.01it/s] 48%|     | 29/60 [00:14<00:15,  1.97it/s] 50%|     | 30/60 [00:15<00:12,  2.47it/s] 52%|    | 31/60 [00:15<00:12,  2.32it/s] 53%|    | 32/60 [00:15<00:14,  1.99it/s] 52%|    | 31/60 [00:15<00:15,  1.90it/s] 53%|    | 32/60 [00:16<00:11,  2.39it/s] 55%|    | 33/60 [00:16<00:11,  2.34it/s] 57%|    | 34/60 [00:16<00:12,  2.08it/s] 55%|    | 33/60 [00:16<00:13,  2.00it/s] 57%|    | 34/60 [00:16<00:10,  2.49it/s] 58%|    | 35/60 [00:17<00:10,  2.40it/s] 60%|    | 36/60 [00:17<00:11,  2.05it/s] 58%|    | 35/60 [00:17<00:12,  1.99it/s] 60%|    | 36/60 [00:17<00:09,  2.46it/s] 62%|   | 37/60 [00:17<00:09,  2.32it/s] 63%|   | 38/60 [00:18<00:10,  2.05it/s] 65%|   | 39/60 [00:18<00:08,  2.41it/s] 62%|   | 37/60 [00:18<00:13,  1.68it/s] 63%|   | 38/60 [00:19<00:10,  2.14it/s] 67%|   | 40/60 [00:19<00:09,  2.05it/s] 68%|   | 41/60 [00:19<00:08,  2.31it/s] 65%|   | 39/60 [00:20<00:12,  1.66it/s] 67%|   | 40/60 [00:20<00:09,  2.12it/s] 70%|   | 42/60 [00:20<00:08,  2.00it/s] 72%|  | 43/60 [00:20<00:07,  2.36it/s] 68%|   | 41/60 [00:20<00:10,  1.78it/s] 70%|   | 42/60 [00:21<00:07,  2.27it/s] 73%|  | 44/60 [00:21<00:07,  2.02it/s] 75%|  | 45/60 [00:21<00:06,  2.39it/s] 72%|  | 43/60 [00:21<00:09,  1.82it/s] 73%|  | 44/60 [00:22<00:06,  2.30it/s] 77%|  | 46/60 [00:22<00:07,  1.94it/s] 78%|  | 47/60 [00:22<00:05,  2.43it/s] 75%|  | 45/60 [00:22<00:08,  1.85it/s] 77%|  | 46/60 [00:23<00:05,  2.34it/s] 80%|  | 48/60 [00:23<00:06,  1.94it/s] 82%| | 49/60 [00:23<00:04,  2.42it/s] 78%|  | 47/60 [00:23<00:06,  1.88it/s] 80%|  | 48/60 [00:24<00:05,  2.36it/s] 83%| | 50/60 [00:24<00:05,  1.89it/s] 85%| | 51/60 [00:24<00:03,  2.39it/s] 82%| | 49/60 [00:24<00:05,  1.94it/s] 83%| | 50/60 [00:24<00:04,  2.43it/s] 87%| | 52/60 [00:25<00:04,  1.95it/s] 88%| | 53/60 [00:25<00:02,  2.42it/s] 85%| | 51/60 [00:25<00:04,  1.91it/s] 87%| | 52/60 [00:25<00:03,  2.39it/s] 90%| | 54/60 [00:26<00:03,  1.97it/s] 92%|| 55/60 [00:26<00:02,  2.49it/s] 88%| | 53/60 [00:26<00:03,  2.00it/s] 90%| | 54/60 [00:26<00:02,  2.50it/s] 93%|| 56/60 [00:26<00:02,  1.96it/s] 95%|| 57/60 [00:27<00:01,  2.46it/s] 92%|| 55/60 [00:27<00:02,  1.97it/s] 93%|| 56/60 [00:27<00:01,  2.48it/s] 97%|| 58/60 [00:27<00:00,  2.02it/s] 98%|| 59/60 [00:28<00:00,  2.51it/s]100%|| 60/60 [00:28<00:00,  2.13it/s]
 95%|| 57/60 [00:28<00:01,  2.09it/s] 97%|| 58/60 [00:28<00:00,  2.60it/s] 98%|| 59/60 [00:29<00:00,  2.11it/s]100%|| 60/60 [00:29<00:00,  2.75it/s]100%|| 60/60 [00:29<00:00,  2.05it/s]
accuracy2:  0.07364705882352941
Testing time 0:00:34
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'test_set_accuracy': 0.07364705882352941}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:17:13,871] torch.distributed.run: [WARNING] 
[2025-11-11 09:17:13,871] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:17:13,871] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:17:13,871] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
  0%|          | 0/11 [00:00<?, ?it/s]Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/11 [00:00<?, ?it/s]  9%|         | 1/11 [00:02<00:23,  2.31s/it]  9%|         | 1/11 [00:02<00:23,  2.31s/it] 18%|        | 2/11 [00:02<00:09,  1.04s/it] 18%|        | 2/11 [00:02<00:09,  1.05s/it] 27%|       | 3/11 [00:02<00:05,  1.57it/s] 27%|       | 3/11 [00:02<00:05,  1.56it/s] 36%|      | 4/11 [00:02<00:03,  2.26it/s] 36%|      | 4/11 [00:02<00:03,  2.25it/s] 45%|     | 5/11 [00:03<00:02,  2.47it/s] 45%|     | 5/11 [00:03<00:02,  2.20it/s] 55%|    | 6/11 [00:03<00:02,  2.34it/s] 55%|    | 6/11 [00:03<00:02,  2.29it/s] 64%|   | 7/11 [00:03<00:01,  2.51it/s] 64%|   | 7/11 [00:04<00:01,  2.28it/s] 73%|  | 8/11 [00:04<00:01,  2.04it/s] 82%| | 9/11 [00:04<00:00,  2.61it/s] 73%|  | 8/11 [00:04<00:01,  1.86it/s] 82%| | 9/11 [00:05<00:00,  2.36it/s] 91%| | 10/11 [00:05<00:00,  2.08it/s]100%|| 11/11 [00:05<00:00,  2.65it/s]100%|| 11/11 [00:05<00:00,  1.94it/s]
 91%| | 10/11 [00:05<00:00,  1.84it/s]100%|| 11/11 [00:05<00:00,  2.35it/s]100%|| 11/11 [00:06<00:00,  1.81it/s]
accuracy2:  0.17974683544303796
Testing time 0:00:07
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'testA_set_accuracy': 0.17974683544303796}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:17:40,958] torch.distributed.run: [WARNING] 
[2025-11-11 09:17:40,958] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:17:40,958] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:17:40,958] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/35 [00:00<?, ?it/s]  0%|          | 0/35 [00:00<?, ?it/s]  3%|         | 1/35 [00:01<01:06,  1.94s/it]  6%|         | 2/35 [00:02<00:31,  1.04it/s]  3%|         | 1/35 [00:02<01:13,  2.16s/it]  9%|         | 3/35 [00:02<00:20,  1.53it/s]  6%|         | 2/35 [00:02<00:34,  1.06s/it] 11%|        | 4/35 [00:02<00:14,  2.10it/s]  9%|         | 3/35 [00:02<00:24,  1.33it/s] 11%|        | 4/35 [00:03<00:17,  1.75it/s] 14%|        | 5/35 [00:03<00:17,  1.67it/s] 17%|        | 6/35 [00:03<00:13,  2.12it/s] 14%|        | 5/35 [00:03<00:18,  1.66it/s] 17%|        | 6/35 [00:04<00:14,  2.01it/s] 20%|        | 7/35 [00:04<00:15,  1.84it/s] 23%|       | 8/35 [00:04<00:12,  2.24it/s] 20%|        | 7/35 [00:04<00:14,  1.96it/s] 23%|       | 8/35 [00:04<00:10,  2.54it/s] 26%|       | 9/35 [00:05<00:11,  2.35it/s] 29%|       | 10/35 [00:05<00:08,  2.95it/s] 26%|       | 9/35 [00:05<00:13,  1.96it/s] 31%|      | 11/35 [00:05<00:10,  2.37it/s] 29%|       | 10/35 [00:05<00:11,  2.26it/s] 34%|      | 12/35 [00:05<00:07,  2.97it/s] 31%|      | 11/35 [00:06<00:11,  2.16it/s] 37%|      | 13/35 [00:06<00:09,  2.35it/s] 34%|      | 12/35 [00:06<00:08,  2.69it/s] 40%|      | 14/35 [00:06<00:07,  2.94it/s] 43%|     | 15/35 [00:07<00:08,  2.44it/s] 37%|      | 13/35 [00:07<00:10,  2.00it/s] 46%|     | 16/35 [00:07<00:06,  3.04it/s] 40%|      | 14/35 [00:07<00:08,  2.54it/s] 49%|     | 17/35 [00:08<00:07,  2.42it/s] 51%|    | 18/35 [00:08<00:05,  2.99it/s] 43%|     | 15/35 [00:08<00:09,  2.08it/s] 46%|     | 16/35 [00:08<00:07,  2.57it/s] 54%|    | 19/35 [00:08<00:06,  2.43it/s] 57%|    | 20/35 [00:08<00:04,  3.02it/s] 49%|     | 17/35 [00:08<00:08,  2.19it/s] 51%|    | 18/35 [00:09<00:06,  2.72it/s] 60%|    | 21/35 [00:09<00:05,  2.46it/s] 63%|   | 22/35 [00:09<00:04,  3.05it/s] 54%|    | 19/35 [00:09<00:07,  2.07it/s] 57%|    | 20/35 [00:09<00:05,  2.61it/s] 66%|   | 23/35 [00:10<00:04,  2.44it/s] 69%|   | 24/35 [00:10<00:03,  3.04it/s] 60%|    | 21/35 [00:10<00:06,  2.17it/s] 63%|   | 22/35 [00:10<00:04,  2.70it/s] 71%|  | 25/35 [00:11<00:04,  2.38it/s] 74%|  | 26/35 [00:11<00:03,  2.97it/s] 66%|   | 23/35 [00:11<00:05,  2.16it/s] 69%|   | 24/35 [00:11<00:04,  2.69it/s] 77%|  | 27/35 [00:11<00:03,  2.39it/s] 80%|  | 28/35 [00:11<00:02,  2.98it/s] 83%| | 29/35 [00:12<00:02,  2.47it/s] 71%|  | 25/35 [00:12<00:04,  2.02it/s] 86%| | 30/35 [00:12<00:01,  3.07it/s] 74%|  | 26/35 [00:12<00:03,  2.55it/s] 89%| | 31/35 [00:13<00:01,  2.49it/s] 77%|  | 27/35 [00:13<00:03,  2.11it/s] 91%|| 32/35 [00:13<00:00,  3.09it/s] 80%|  | 28/35 [00:13<00:02,  2.63it/s] 94%|| 33/35 [00:13<00:00,  2.49it/s] 97%|| 34/35 [00:14<00:00,  3.09it/s] 83%| | 29/35 [00:14<00:02,  2.08it/s]100%|| 35/35 [00:14<00:00,  2.46it/s]
 86%| | 30/35 [00:14<00:01,  2.63it/s] 89%| | 31/35 [00:15<00:01,  2.01it/s] 91%|| 32/35 [00:15<00:01,  2.54it/s] 94%|| 33/35 [00:15<00:00,  2.10it/s] 97%|| 34/35 [00:15<00:00,  2.63it/s]100%|| 35/35 [00:16<00:00,  2.17it/s]
accuracy2:  0.06443719412724307
Testing time 0:00:18
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'testB_set_accuracy': 0.06443719412724307}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:18:18,053] torch.distributed.run: [WARNING] 
[2025-11-11 09:18:18,053] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:18:18,053] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:18:18,053] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/36 [00:00<?, ?it/s]  0%|          | 0/36 [00:00<?, ?it/s]  3%|         | 1/36 [00:01<00:58,  1.67s/it]  3%|         | 1/36 [00:01<01:02,  1.78s/it]  6%|         | 2/36 [00:01<00:26,  1.28it/s]  6%|         | 2/36 [00:01<00:28,  1.21it/s]  8%|         | 3/36 [00:01<00:16,  2.02it/s]  8%|         | 3/36 [00:02<00:17,  1.91it/s] 11%|         | 4/36 [00:02<00:13,  2.34it/s] 14%|        | 5/36 [00:02<00:11,  2.71it/s] 11%|         | 4/36 [00:02<00:17,  1.79it/s] 14%|        | 5/36 [00:02<00:13,  2.27it/s] 17%|        | 6/36 [00:03<00:13,  2.16it/s] 19%|        | 7/36 [00:03<00:10,  2.76it/s] 17%|        | 6/36 [00:03<00:15,  1.95it/s] 19%|        | 7/36 [00:03<00:11,  2.44it/s] 22%|       | 8/36 [00:03<00:11,  2.51it/s] 25%|       | 9/36 [00:04<00:08,  3.10it/s] 22%|       | 8/36 [00:04<00:13,  2.07it/s] 28%|       | 10/36 [00:04<00:09,  2.68it/s] 25%|       | 9/36 [00:04<00:10,  2.62it/s] 31%|       | 11/36 [00:04<00:07,  3.26it/s] 33%|      | 12/36 [00:05<00:08,  2.82it/s] 28%|       | 10/36 [00:05<00:11,  2.26it/s] 36%|      | 13/36 [00:05<00:06,  3.39it/s] 31%|       | 11/36 [00:05<00:08,  2.82it/s] 39%|      | 14/36 [00:05<00:07,  2.87it/s] 42%|     | 15/36 [00:05<00:06,  3.44it/s] 33%|      | 12/36 [00:06<00:10,  2.22it/s] 36%|      | 13/36 [00:06<00:08,  2.75it/s] 44%|     | 16/36 [00:06<00:07,  2.81it/s] 47%|     | 17/36 [00:06<00:05,  3.38it/s] 39%|      | 14/36 [00:06<00:09,  2.31it/s] 42%|     | 15/36 [00:06<00:07,  2.80it/s] 50%|     | 18/36 [00:07<00:06,  2.78it/s] 53%|    | 19/36 [00:07<00:05,  3.35it/s] 44%|     | 16/36 [00:07<00:08,  2.28it/s] 56%|    | 20/36 [00:07<00:05,  2.81it/s] 47%|     | 17/36 [00:07<00:06,  2.78it/s] 58%|    | 21/36 [00:07<00:04,  3.38it/s] 50%|     | 18/36 [00:08<00:08,  2.23it/s] 61%|    | 22/36 [00:08<00:05,  2.72it/s] 53%|    | 19/36 [00:08<00:06,  2.76it/s] 64%|   | 23/36 [00:08<00:03,  3.27it/s] 56%|    | 20/36 [00:09<00:06,  2.36it/s] 67%|   | 24/36 [00:09<00:04,  2.57it/s] 58%|    | 21/36 [00:09<00:05,  2.92it/s] 69%|   | 25/36 [00:09<00:03,  3.12it/s] 72%|  | 26/36 [00:10<00:04,  2.06it/s] 61%|    | 22/36 [00:10<00:07,  1.92it/s] 75%|  | 27/36 [00:10<00:03,  2.58it/s] 64%|   | 23/36 [00:10<00:05,  2.44it/s] 78%|  | 28/36 [00:11<00:04,  1.82it/s] 67%|   | 24/36 [00:11<00:06,  1.78it/s] 81%|  | 29/36 [00:11<00:03,  2.32it/s] 69%|   | 25/36 [00:11<00:04,  2.29it/s] 72%|  | 26/36 [00:12<00:05,  1.72it/s] 83%| | 30/36 [00:12<00:03,  1.73it/s] 75%|  | 27/36 [00:12<00:04,  2.22it/s] 86%| | 31/36 [00:12<00:02,  2.21it/s] 78%|  | 28/36 [00:13<00:04,  1.71it/s] 89%| | 32/36 [00:13<00:02,  1.71it/s] 92%|| 33/36 [00:13<00:01,  2.20it/s] 81%|  | 29/36 [00:13<00:03,  2.17it/s] 94%|| 34/36 [00:14<00:01,  1.86it/s] 83%| | 30/36 [00:14<00:03,  1.84it/s] 97%|| 35/36 [00:14<00:00,  2.37it/s] 86%| | 31/36 [00:14<00:02,  2.32it/s]100%|| 36/36 [00:14<00:00,  3.06it/s]100%|| 36/36 [00:14<00:00,  2.46it/s]
 89%| | 32/36 [00:15<00:02,  1.78it/s] 92%|| 33/36 [00:15<00:01,  2.08it/s] 94%|| 34/36 [00:16<00:01,  1.99it/s] 97%|| 35/36 [00:16<00:00,  2.41it/s]100%|| 36/36 [00:16<00:00,  2.17it/s]
accuracy2:  0.005098039215686275
Testing time 0:00:19
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'testC_set_accuracy': 0.005098039215686275}
