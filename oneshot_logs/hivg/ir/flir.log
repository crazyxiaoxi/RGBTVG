/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:10:51,319] torch.distributed.run: [WARNING] 
[2025-11-11 09:10:51,319] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:10:51,319] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:10:51,319] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 1): env://
| distributed init (rank 0): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/9 [00:00<?, ?it/s]  0%|          | 0/9 [00:00<?, ?it/s] 11%|         | 1/9 [00:01<00:09,  1.15s/it] 11%|         | 1/9 [00:01<00:09,  1.19s/it] 22%|       | 2/9 [00:01<00:04,  1.62it/s] 22%|       | 2/9 [00:01<00:04,  1.55it/s] 33%|      | 3/9 [00:01<00:02,  2.25it/s] 33%|      | 3/9 [00:01<00:02,  2.07it/s] 44%|     | 4/9 [00:01<00:01,  2.61it/s] 44%|     | 4/9 [00:01<00:01,  2.73it/s] 56%|    | 5/9 [00:02<00:01,  2.92it/s] 56%|    | 5/9 [00:02<00:01,  3.14it/s] 67%|   | 6/9 [00:02<00:00,  3.11it/s] 67%|   | 6/9 [00:02<00:00,  3.25it/s] 78%|  | 7/9 [00:02<00:00,  3.22it/s] 78%|  | 7/9 [00:02<00:00,  3.48it/s] 89%| | 8/9 [00:03<00:00,  3.47it/s] 89%| | 8/9 [00:02<00:00,  3.48it/s]100%|| 9/9 [00:03<00:00,  4.11it/s]100%|| 9/9 [00:03<00:00,  4.10it/s]100%|| 9/9 [00:03<00:00,  2.79it/s]
100%|| 9/9 [00:03<00:00,  2.77it/s]
accuracy2:  0.10855263157894737
Testing time 0:00:03
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'val_set_accuracy': 0.10855263157894737}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:11:13,508] torch.distributed.run: [WARNING] 
[2025-11-11 09:11:13,508] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:11:13,508] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:11:13,508] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 1): env://
| distributed init (rank 0): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
  0%|          | 0/30 [00:00<?, ?it/s]Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/30 [00:00<?, ?it/s]  3%|         | 1/30 [00:01<00:31,  1.10s/it]  3%|         | 1/30 [00:01<00:35,  1.23s/it]  7%|         | 2/30 [00:01<00:15,  1.86it/s]  7%|         | 2/30 [00:01<00:16,  1.65it/s] 10%|         | 3/30 [00:01<00:09,  2.79it/s] 10%|         | 3/30 [00:01<00:10,  2.52it/s] 13%|        | 4/30 [00:01<00:07,  3.65it/s] 17%|        | 5/30 [00:01<00:05,  4.39it/s] 13%|        | 4/30 [00:01<00:07,  3.31it/s] 20%|        | 6/30 [00:01<00:04,  5.01it/s] 17%|        | 5/30 [00:01<00:06,  4.06it/s] 23%|       | 7/30 [00:01<00:04,  5.46it/s] 20%|        | 6/30 [00:02<00:05,  4.68it/s] 27%|       | 8/30 [00:02<00:03,  5.85it/s] 23%|       | 7/30 [00:02<00:04,  5.20it/s] 30%|       | 9/30 [00:02<00:03,  6.05it/s] 27%|       | 8/30 [00:02<00:04,  5.35it/s] 33%|      | 10/30 [00:02<00:03,  6.08it/s] 30%|       | 9/30 [00:02<00:03,  5.63it/s] 33%|      | 10/30 [00:02<00:03,  5.64it/s] 37%|      | 11/30 [00:02<00:03,  5.57it/s] 40%|      | 12/30 [00:02<00:03,  5.93it/s] 37%|      | 11/30 [00:02<00:03,  5.86it/s] 40%|      | 12/30 [00:02<00:03,  5.95it/s] 43%|     | 13/30 [00:03<00:03,  5.27it/s] 43%|     | 13/30 [00:03<00:02,  6.09it/s] 47%|     | 14/30 [00:03<00:02,  5.68it/s] 47%|     | 14/30 [00:03<00:03,  5.18it/s] 50%|     | 15/30 [00:03<00:02,  5.17it/s] 50%|     | 15/30 [00:03<00:02,  5.60it/s] 53%|    | 16/30 [00:03<00:02,  5.60it/s] 57%|    | 17/30 [00:03<00:02,  5.52it/s] 53%|    | 16/30 [00:03<00:03,  4.39it/s] 60%|    | 18/30 [00:03<00:02,  5.87it/s] 57%|    | 17/30 [00:04<00:02,  4.85it/s] 63%|   | 19/30 [00:04<00:01,  5.54it/s] 67%|   | 20/30 [00:04<00:01,  5.86it/s] 60%|    | 18/30 [00:04<00:02,  4.52it/s] 63%|   | 19/30 [00:04<00:02,  5.03it/s] 70%|   | 21/30 [00:04<00:01,  5.36it/s] 67%|   | 20/30 [00:04<00:01,  5.39it/s] 73%|  | 22/30 [00:04<00:01,  5.77it/s] 70%|   | 21/30 [00:04<00:01,  5.71it/s] 77%|  | 23/30 [00:04<00:01,  5.31it/s] 73%|  | 22/30 [00:04<00:01,  5.83it/s] 80%|  | 24/30 [00:04<00:01,  5.60it/s] 77%|  | 23/30 [00:05<00:01,  6.01it/s] 83%| | 25/30 [00:05<00:00,  5.43it/s] 80%|  | 24/30 [00:05<00:01,  5.81it/s] 87%| | 26/30 [00:05<00:00,  5.76it/s] 83%| | 25/30 [00:05<00:00,  6.00it/s] 90%| | 27/30 [00:05<00:00,  5.84it/s] 87%| | 26/30 [00:05<00:00,  5.98it/s] 93%|| 28/30 [00:05<00:00,  5.96it/s] 90%| | 27/30 [00:05<00:00,  6.22it/s] 97%|| 29/30 [00:05<00:00,  5.99it/s] 93%|| 28/30 [00:05<00:00,  6.12it/s]100%|| 30/30 [00:05<00:00,  6.80it/s] 97%|| 29/30 [00:06<00:00,  6.18it/s]100%|| 30/30 [00:06<00:00,  4.95it/s]
100%|| 30/30 [00:06<00:00,  6.67it/s]100%|| 30/30 [00:06<00:00,  4.80it/s]
accuracy2:  0.13098591549295774
Testing time 0:00:08
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'test_set_accuracy': 0.13098591549295774}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:11:40,996] torch.distributed.run: [WARNING] 
[2025-11-11 09:11:40,996] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:11:40,996] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:11:40,996] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 1): env://
| distributed init (rank 0): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/12 [00:00<?, ?it/s]  0%|          | 0/12 [00:00<?, ?it/s]  8%|         | 1/12 [00:01<00:14,  1.29s/it]  8%|         | 1/12 [00:01<00:14,  1.31s/it] 17%|        | 2/12 [00:01<00:06,  1.60it/s] 17%|        | 2/12 [00:01<00:06,  1.57it/s] 25%|       | 3/12 [00:01<00:03,  2.43it/s] 25%|       | 3/12 [00:01<00:03,  2.37it/s] 33%|      | 4/12 [00:01<00:02,  3.26it/s] 33%|      | 4/12 [00:01<00:02,  3.20it/s] 42%|     | 5/12 [00:01<00:01,  4.01it/s] 42%|     | 5/12 [00:01<00:01,  3.91it/s] 50%|     | 6/12 [00:02<00:01,  4.62it/s] 50%|     | 6/12 [00:02<00:01,  4.44it/s] 58%|    | 7/12 [00:02<00:01,  4.96it/s] 58%|    | 7/12 [00:02<00:01,  4.83it/s] 67%|   | 8/12 [00:02<00:00,  4.66it/s] 67%|   | 8/12 [00:02<00:00,  5.31it/s] 75%|  | 9/12 [00:02<00:00,  5.12it/s] 75%|  | 9/12 [00:02<00:00,  5.54it/s] 83%| | 10/12 [00:02<00:00,  5.70it/s] 83%| | 10/12 [00:02<00:00,  4.76it/s] 92%|| 11/12 [00:03<00:00,  5.23it/s] 92%|| 11/12 [00:03<00:00,  4.76it/s]100%|| 12/12 [00:03<00:00,  6.09it/s]100%|| 12/12 [00:03<00:00,  3.75it/s]
100%|| 12/12 [00:03<00:00,  5.64it/s]100%|| 12/12 [00:03<00:00,  3.69it/s]
accuracy2:  0.2064439140811456
Testing time 0:00:04
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'testA_set_accuracy': 0.2064439140811456}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:12:03,027] torch.distributed.run: [WARNING] 
[2025-11-11 09:12:03,027] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:12:03,027] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:12:03,027] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/9 [00:00<?, ?it/s]  0%|          | 0/9 [00:00<?, ?it/s] 11%|         | 1/9 [00:01<00:11,  1.41s/it] 11%|         | 1/9 [00:01<00:10,  1.36s/it] 22%|       | 2/9 [00:01<00:04,  1.48it/s] 22%|       | 2/9 [00:01<00:04,  1.51it/s] 33%|      | 3/9 [00:01<00:02,  2.23it/s] 44%|     | 4/9 [00:01<00:01,  3.00it/s] 33%|      | 3/9 [00:01<00:02,  2.15it/s] 56%|    | 5/9 [00:02<00:01,  3.70it/s] 44%|     | 4/9 [00:02<00:01,  2.53it/s] 67%|   | 6/9 [00:02<00:00,  4.28it/s] 78%|  | 7/9 [00:02<00:00,  4.72it/s] 56%|    | 5/9 [00:02<00:01,  2.76it/s] 89%| | 8/9 [00:02<00:00,  4.65it/s]100%|| 9/9 [00:02<00:00,  5.14it/s] 67%|   | 6/9 [00:02<00:01,  2.95it/s]100%|| 9/9 [00:02<00:00,  3.10it/s]
 78%|  | 7/9 [00:02<00:00,  3.46it/s] 89%| | 8/9 [00:03<00:00,  3.85it/s]100%|| 9/9 [00:03<00:00,  4.43it/s]100%|| 9/9 [00:03<00:00,  2.76it/s]
accuracy2:  0.159375
Testing time 0:00:04
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'testB_set_accuracy': 0.159375}
/home/xijiawen/anaconda3/envs/rgbtvg/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2025-11-11 09:12:25,161] torch.distributed.run: [WARNING] 
[2025-11-11 09:12:25,161] torch.distributed.run: [WARNING] *****************************************
[2025-11-11 09:12:25,161] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-11-11 09:12:25,161] torch.distributed.run: [WARNING] *****************************************
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
/home/xijiawen/code/rgbtvg/RGBTVG/models/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
| distributed init (rank 0): env://
| distributed init (rank 1): env://
git:
  sha: 98f4a1d7ef7255e0eecf5a63a86b57e17cad337d, status: has uncommited changes, branch: main

Building HiVG model...
init HiVG model...
init CLIP ViT-B/16

extract vision layer:  [1, 4, 8, 12]
extract text layer:  [12]
image size:  224  *  224
adapt_layer:  [0, 3, 7, 11]
trainable params: 4,718,592 || all params: 184,418,305 || trainable%: 2.5586
trainable params: 0 || all params: 184,418,305 || trainable%: 0.0000
Open Multi-layer Adaptive Cross-modal Bridge parameters ...
param name:  0.cross_norm.weight
param name:  0.cross_norm.bias
param name:  0.cross_attn.k_proj.base_layer.weight
param name:  0.cross_attn.k_proj.base_layer.bias
param name:  0.cross_attn.k_proj.lora_A.default.weight
param name:  0.cross_attn.k_proj.lora_B.default.weight
param name:  0.cross_attn.v_proj.base_layer.weight
param name:  0.cross_attn.v_proj.base_layer.bias
param name:  0.cross_attn.v_proj.lora_A.default.weight
param name:  0.cross_attn.v_proj.lora_B.default.weight
param name:  0.cross_attn.q_proj.base_layer.weight
param name:  0.cross_attn.q_proj.base_layer.bias
param name:  0.cross_attn.q_proj.lora_A.default.weight
param name:  0.cross_attn.q_proj.lora_B.default.weight
param name:  0.cross_attn.out_proj.base_layer.weight
param name:  0.cross_attn.out_proj.base_layer.bias
param name:  0.cross_attn.out_proj.lora_A.default.weight
param name:  0.cross_attn.out_proj.lora_B.default.weight
param name:  0.cross_mlp.fc1.weight
param name:  0.cross_mlp.fc1.bias
param name:  0.cross_mlp.fc2.weight
param name:  0.cross_mlp.fc2.bias
param name:  0.cross_gate.weight
param name:  0.cross_gate.bias
param name:  0.cross_adaptive_weights.0.weight
param name:  3.cross_norm.weight
param name:  3.cross_norm.bias
param name:  3.cross_attn.k_proj.base_layer.weight
param name:  3.cross_attn.k_proj.base_layer.bias
param name:  3.cross_attn.k_proj.lora_A.default.weight
param name:  3.cross_attn.k_proj.lora_B.default.weight
param name:  3.cross_attn.v_proj.base_layer.weight
param name:  3.cross_attn.v_proj.base_layer.bias
param name:  3.cross_attn.v_proj.lora_A.default.weight
param name:  3.cross_attn.v_proj.lora_B.default.weight
param name:  3.cross_attn.q_proj.base_layer.weight
param name:  3.cross_attn.q_proj.base_layer.bias
param name:  3.cross_attn.q_proj.lora_A.default.weight
param name:  3.cross_attn.q_proj.lora_B.default.weight
param name:  3.cross_attn.out_proj.base_layer.weight
param name:  3.cross_attn.out_proj.base_layer.bias
param name:  3.cross_attn.out_proj.lora_A.default.weight
param name:  3.cross_attn.out_proj.lora_B.default.weight
param name:  3.cross_mlp.fc1.weight
param name:  3.cross_mlp.fc1.bias
param name:  3.cross_mlp.fc2.weight
param name:  3.cross_mlp.fc2.bias
param name:  3.cross_gate.weight
param name:  3.cross_gate.bias
param name:  3.cross_adaptive_weights.0.weight
param name:  7.cross_norm.weight
param name:  7.cross_norm.bias
param name:  7.cross_attn.k_proj.base_layer.weight
param name:  7.cross_attn.k_proj.base_layer.bias
param name:  7.cross_attn.k_proj.lora_A.default.weight
param name:  7.cross_attn.k_proj.lora_B.default.weight
param name:  7.cross_attn.v_proj.base_layer.weight
param name:  7.cross_attn.v_proj.base_layer.bias
param name:  7.cross_attn.v_proj.lora_A.default.weight
param name:  7.cross_attn.v_proj.lora_B.default.weight
param name:  7.cross_attn.q_proj.base_layer.weight
param name:  7.cross_attn.q_proj.base_layer.bias
param name:  7.cross_attn.q_proj.lora_A.default.weight
param name:  7.cross_attn.q_proj.lora_B.default.weight
param name:  7.cross_attn.out_proj.base_layer.weight
param name:  7.cross_attn.out_proj.base_layer.bias
param name:  7.cross_attn.out_proj.lora_A.default.weight
param name:  7.cross_attn.out_proj.lora_B.default.weight
param name:  7.cross_mlp.fc1.weight
param name:  7.cross_mlp.fc1.bias
param name:  7.cross_mlp.fc2.weight
param name:  7.cross_mlp.fc2.bias
param name:  7.cross_gate.weight
param name:  7.cross_gate.bias
param name:  7.cross_adaptive_weights.0.weight
param name:  11.cross_norm.weight
param name:  11.cross_norm.bias
param name:  11.cross_attn.k_proj.base_layer.weight
param name:  11.cross_attn.k_proj.base_layer.bias
param name:  11.cross_attn.k_proj.lora_A.default.weight
param name:  11.cross_attn.k_proj.lora_B.default.weight
param name:  11.cross_attn.v_proj.base_layer.weight
param name:  11.cross_attn.v_proj.base_layer.bias
param name:  11.cross_attn.v_proj.lora_A.default.weight
param name:  11.cross_attn.v_proj.lora_B.default.weight
param name:  11.cross_attn.q_proj.base_layer.weight
param name:  11.cross_attn.q_proj.base_layer.bias
param name:  11.cross_attn.q_proj.lora_A.default.weight
param name:  11.cross_attn.q_proj.lora_B.default.weight
param name:  11.cross_attn.out_proj.base_layer.weight
param name:  11.cross_attn.out_proj.base_layer.bias
param name:  11.cross_attn.out_proj.lora_A.default.weight
param name:  11.cross_attn.out_proj.lora_B.default.weight
param name:  11.cross_mlp.fc1.weight
param name:  11.cross_mlp.fc1.bias
param name:  11.cross_mlp.fc2.weight
param name:  11.cross_mlp.fc2.bias
param name:  11.cross_gate.weight
param name:  11.cross_gate.bias
param name:  11.cross_adaptive_weights.0.weight
trainable params: 30,865,408 || all params: 184,418,305 || trainable%: 16.7366

normalize_before:  True
number of all params:  213244677
Missing keys when loading stage model: 
 ['clip.base_model.model.logit_scale', 'clip.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.base_layer.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.visual_projection.weight', 'clip.base_model.model.text_projection.weight']
Unexpected additional keys when loading stage model: 
 ['clip.base_model.model.base_model.model.base_model.model.base_model.model.logit_scale', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.token_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_model.final_layer_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.class_embedding', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_ids', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.patch_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.embeddings.position_embedding.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.pre_layrnorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_norm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.k_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.v_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.q_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_A.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_attn.out_proj.lora_B.default.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_gate.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.cross_adaptive_weights.0.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.vision_model.post_layernorm.bias', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.visual_projection.weight', 'clip.base_model.model.base_model.model.base_model.model.base_model.model.text_projection.weight']
Current model training epoch is:  1
  0%|          | 0/14 [00:00<?, ?it/s]  0%|          | 0/14 [00:00<?, ?it/s]  7%|         | 1/14 [00:01<00:18,  1.44s/it]  7%|         | 1/14 [00:01<00:19,  1.47s/it] 14%|        | 2/14 [00:01<00:08,  1.46it/s] 14%|        | 2/14 [00:01<00:08,  1.43it/s] 21%|       | 3/14 [00:01<00:04,  2.24it/s] 21%|       | 3/14 [00:01<00:05,  2.20it/s] 29%|       | 4/14 [00:01<00:03,  2.99it/s] 29%|       | 4/14 [00:01<00:03,  2.94it/s] 36%|      | 5/14 [00:02<00:02,  3.59it/s] 36%|      | 5/14 [00:02<00:02,  3.58it/s] 43%|     | 6/14 [00:02<00:01,  4.15it/s] 43%|     | 6/14 [00:02<00:01,  4.19it/s] 50%|     | 7/14 [00:02<00:01,  4.67it/s] 50%|     | 7/14 [00:02<00:01,  4.68it/s] 57%|    | 8/14 [00:02<00:01,  4.96it/s] 57%|    | 8/14 [00:02<00:01,  5.03it/s] 64%|   | 9/14 [00:02<00:00,  5.14it/s] 64%|   | 9/14 [00:02<00:01,  4.55it/s] 71%|  | 10/14 [00:03<00:00,  4.96it/s] 71%|  | 10/14 [00:03<00:00,  4.23it/s] 79%|  | 11/14 [00:03<00:00,  5.18it/s] 79%|  | 11/14 [00:03<00:00,  4.58it/s] 86%| | 12/14 [00:03<00:00,  5.31it/s] 86%| | 12/14 [00:03<00:00,  4.35it/s] 93%|| 13/14 [00:03<00:00,  5.48it/s]100%|| 14/14 [00:03<00:00,  5.99it/s] 93%|| 13/14 [00:03<00:00,  4.79it/s]100%|| 14/14 [00:03<00:00,  3.69it/s]
100%|| 14/14 [00:03<00:00,  5.40it/s]100%|| 14/14 [00:03<00:00,  3.53it/s]
accuracy2:  0.008113590263691683
Testing time 0:00:05
{'test_model:': '../dataset_and_pretrain_model/pretrain_model/pretrained_weights/HiVG/mixup_pretraining_base/best_checkpoint.pth', 'testC_set_accuracy': 0.008113590263691683}
